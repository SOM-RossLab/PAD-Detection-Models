{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting up the python environment\n",
    "import pandas as pd\n",
    "import os \n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "%matplotlib inline \n",
    "import textwrap\n",
    "\n",
    "##Set up BQ API\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "##Set up Google sdk environment\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '***'",
    "## Connect to the right GCP project\n",
    "os.environ['GCLOUD_PROJECT'] = '***' \n",
    "%load_ext google.cloud.bigquery\n",
    "client=bigquery.Client()\n",
    "\n",
    "project_id = \"***\"\n",
    "dataset_id = \"***\"\n",
    "work_project_id = '***' \n",
    "work_dataset_id = '***'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079870ca",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f112cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 1\n",
    "query=\"\"\"\n",
    "(SELECT a.* EXCEPT (PAD, Smoking_status), 1 AS label\n",
    "FROM `{work_project_id}.{work_dataset_id}.Cases_FM_temp_smok` a\n",
    "JOIN `{work_project_id}.{work_dataset_id}.person_ids_cases_sequence_fold_{fold_num}` b\n",
    "ON a.person_id = b.person_id\n",
    "WHERE a.person_id NOT IN (\n",
    "SELECT *\n",
    "FROM `{work_project_id}.{work_dataset_id}.321052_ids`)\n",
    ")\n",
    "UNION ALL\n",
    "(SELECT a.* EXCEPT (PAD, Smoking_status), 0 AS label\n",
    "FROM `{work_project_id}.{work_dataset_id}.Controls_FM_temp_smok` a\n",
    "JOIN `{work_project_id}.{work_dataset_id}.person_ids_controls_sequence_fold_{fold_num}` b\n",
    "ON a.person_id = b.person_id)\n",
    "\"\"\".format_map({\n",
    "                'project_id': project_id,\n",
    "                'dataset_id': dataset_id,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "                'fold_num': fold_num})\n",
    "df = client.query(query).to_dataframe()\n",
    "df\n",
    "df.to_csv(\"data_{fold_num}.csv\".format_map({'fold_num': fold_num}), index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing\n",
    "df_ids = pd.read_csv(\"/home/jupyter/PAD_npj/random_forest/cv/person_ids/testids_fold{fold_num}.csv\".format_map({'fold_num': fold_num}))\n",
    "df_ids = df_ids.drop_duplicates(subset=['test_person_id'])\n",
    "df_ids.head()\n",
    "\n",
    "df_test = df.loc[df['person_id'].isin(df_ids['test_person_id'])].drop_duplicates(subset=['person_id'])\n",
    "len(df_test)\n",
    "\n",
    "sort_mapping = df_ids.reset_index().set_index('test_person_id')\n",
    "sort_mapping\n",
    "\n",
    "df_test['person_id_num'] = df_test['person_id'].map(sort_mapping['index'])\n",
    "df_test\n",
    "\n",
    "df_test = df_test.sort_values('person_id_num')\n",
    "df_test\n",
    "\n",
    "df_test = df_test.drop(columns=['person_id_num'])\n",
    "df_test\n",
    "\n",
    "df_test = df_test.reset_index().drop(columns=['index'])\n",
    "df_test\n",
    "\n",
    "df_test = df_test.drop(\"person_id\",axis = 1)\n",
    "df_test[\"Female\"] = df_test[\"Female\"].astype(\"category\")\n",
    "df_test[\"Race\"] = df_test[\"Race\"].astype(\"category\")\n",
    "df_test[\"CVA\"] = df_test[\"CVA\"].astype(\"category\")\n",
    "df_test[\"CAD\"] = df_test[\"CAD\"].astype(\"category\")\n",
    "df_test[\"HF\"] = df_test[\"HF\"].astype(\"category\")\n",
    "df_test[\"HTN\"] = df_test[\"HTN\"].astype(\"category\")\n",
    "df_test[\"Diab\"] = df_test[\"Diab\"].astype(\"category\")\n",
    "df_test[\"HLD\"] = df_test[\"HLD\"].astype(\"category\")\n",
    "df_test[\"new_Smoking_status\"] = df_test[\"new_Smoking_status\"].astype(\"category\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_test[\"Race\"])\n",
    "list(le.classes_)\n",
    "df_test[\"Race\"] = le.transform(df_test[\"Race\"])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_test[\"new_Smoking_status\"])\n",
    "list(le.classes_)\n",
    "df_test[\"new_Smoking_status\"] = le.transform(df_test[\"new_Smoking_status\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db99b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop(\"label\",axis = 1) \n",
    "y_test = df_test.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4716ef",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc397f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1 = pd.read_csv(\"data_2.csv\")\n",
    "fold2 = pd.read_csv(\"data_3.csv\")\n",
    "fold3 = pd.read_csv(\"data_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29237c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe for training feature dataset\n",
    "df_train = pd.concat([fold1, fold2, fold3]) \n",
    "df_train = df_train.drop(\"person_id\",axis = 1)\n",
    "df_train[\"Female\"] = df_train[\"Female\"].astype(\"category\")\n",
    "df_train[\"Race\"] = df_train[\"Race\"].astype(\"category\")\n",
    "df_train[\"CVA\"] = df_train[\"CVA\"].astype(\"category\")\n",
    "df_train[\"CAD\"] = df_train[\"CAD\"].astype(\"category\")\n",
    "df_train[\"HF\"] = df_train[\"HF\"].astype(\"category\")\n",
    "df_train[\"HTN\"] = df_train[\"HTN\"].astype(\"category\")\n",
    "df_train[\"Diab\"] = df_train[\"Diab\"].astype(\"category\")\n",
    "df_train[\"HLD\"] = df_train[\"HLD\"].astype(\"category\")\n",
    "df_train[\"new_Smoking_status\"] = df_train[\"new_Smoking_status\"].astype(\"category\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train[\"Race\"])\n",
    "list(le.classes_)\n",
    "df_train[\"Race\"] = le.transform(df_train[\"Race\"])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train[\"new_Smoking_status\"])\n",
    "list(le.classes_)\n",
    "df_train[\"new_Smoking_status\"] = le.transform(df_train[\"new_Smoking_status\"])\n",
    "df_train = df_train.reset_index().drop(columns=['index'])\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "X_train = df_train.drop(\"label\",axis = 1) \n",
    "y_train = df_train.label\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba685ff",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888fda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032662eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_regression.predict_proba(X_test)[:,1]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137960e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_pred)\n",
    "auc = auc(fpr, tpr)\n",
    "auc\n",
    "\n",
    "fold_num_print = 'Fold_num_1'\n",
    "fold_title = 'Fold num 1'\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('LR - ROC curve - ' + str(fold_title))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('PAD_LR_auc_curve_'+str(fold_num_print)+'.jpeg')\n",
    "plt.show()\n",
    "\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(y_test, y_pred, n_bins=10)\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "plt.plot(mpv, fop, marker='.')\n",
    "plt.xlabel('True probabilities')\n",
    "plt.ylabel('Predicted probabilities')\n",
    "plt.title('LR - Calibration curve - '+ str(fold_title))\n",
    "plt.savefig('PAD_LR_calibration_'+fold_num_print+'.jpeg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "optimal_threshold = thresholds[ix]\n",
    "print(\"optimal_threshold = \", optimal_threshold)\n",
    "\n",
    "cm=confusion_matrix(y_test, y_pred>optimal_threshold)\n",
    "TP = cm[0][0]\n",
    "FN = cm[0][1]\n",
    "FP = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "cm,TP,TN,FP,FN,auc,TNR,TPR\n",
    "\n",
    "print(\"AUC = \", auc)\n",
    "print(\"Specificity = \", TNR)\n",
    "print(\"Sensitivity = \", TPR)\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "alpha = .95\n",
    "# y_pred = np.array([0.21, 0.32, 0.63, 0.35, 0.92, 0.79, 0.82, 0.99, 0.04])\n",
    "y_true = y_test\n",
    "\n",
    "auc_delong, auc_cov = delong_roc_variance(\n",
    "    y_true,\n",
    "    y_pred)\n",
    "\n",
    "auc_std = np.sqrt(auc_cov)\n",
    "lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "ci = stats.norm.ppf(\n",
    "    lower_upper_q,\n",
    "    loc=auc_delong,\n",
    "    scale=auc_std)\n",
    "\n",
    "ci[ci > 1] = 1\n",
    "\n",
    "print('AUC:', auc_delong)\n",
    "# print('AUC COV:', auc_cov)\n",
    "print('95% AUC CI:', ci)\n",
    "\n",
    "d = {'auc': auc, 'specificity': TNR, 'sensitivity': TPR}\n",
    "df = pd.DataFrame(data=d, index = [0], dtype=np.float64)\n",
    "df.to_csv('auc_specificity_sensitivity_fold1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af43a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 1\n",
    "pd.DataFrame(fpr).to_csv(\"plots/fpr\" + str(fold_num) + \".csv\")\n",
    "pd.DataFrame(tpr).to_csv(\"plots/tpr\" + str(fold_num) + \".csv\")\n",
    "pd.DataFrame(fop).to_csv(\"plots/fop\" + str(fold_num) + \".csv\")\n",
    "pd.DataFrame(mpv).to_csv(\"plots/mpv\" + str(fold_num) + \".csv\")\n",
    "pd.DataFrame([auc]).to_csv(\"plots/auc\" + str(fold_num) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e40a639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3333a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b29b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
