{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting up the python environment\n",
    "import pandas as pd\n",
    "import os \n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "%matplotlib inline \n",
    "import textwrap\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "##Setting up BQ API\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "##Set up Google sdk environment\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = ***\n",
    "## Connect to the right GCP project\n",
    "os.environ['GCLOUD_PROJECT'] = '***' \n",
    "%load_ext google.cloud.bigquery\n",
    "client=bigquery.Client()\n",
    "\n",
    "project_id = \"***\"\n",
    "dataset_id = \"***\"\n",
    "work_project_id = '***' \n",
    "work_dataset_id = '***'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd6607d",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ee4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "excludeIds = []\n",
    "for i in range(5):\n",
    "    excludeIds.append(pd.read_csv(\"321052_ids/321052_ids_fold\" + str(i + 1) + \".csv\"))\n",
    "excludeIds = pd.concat(excludeIds)\n",
    "excludeIds.to_csv('321052_ids/321052_ids.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing test set\n",
    "cases = pd.read_csv(\"Cases_sequence_fold1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c74b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723bb0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = pd.read_csv(\"Cases_sequence_fold1.csv\")\n",
    "cases = cases[~cases['person_id'].isin(excludeIds['person_id'])]\n",
    "controls = pd.read_csv(\"Controls_sequence_fold1.csv\")\n",
    "controls = controls.dropna()\n",
    "\n",
    "Xcases = cases.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycases = np.ones(len(Xcases.index), dtype=np.int64)\n",
    "\n",
    "Xcontrols = controls.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycontrols = np.zeros(len(Xcontrols.index), dtype=np.int64)\n",
    "\n",
    "X = np.concatenate((Xcases, Xcontrols))\n",
    "y = np.concatenate((ycases, ycontrols))\n",
    "\n",
    "mm = -1\n",
    "print(f'Number of features {mm}')\n",
    "for lis in X:\n",
    "    if len(lis) > mm:\n",
    "        mm = len(lis)\n",
    "mm\n",
    "XX = np.zeros((len(X), mm), dtype=np.int64)\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X[i])):\n",
    "        XX[i, j] = int(X[i][j])\n",
    "print(f'Number of features {mm}')\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_test, y_test = shuffle(XX, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf28447f",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing validation set\n",
    "cases_val = pd.read_csv(\"Cases_sequence_fold2.csv\")\n",
    "cases_val = cases_val[~cases_val['person_id'].isin(excludeIds['person_id'])]\n",
    "controls_val = pd.read_csv(\"Controls_sequence_fold2.csv\",engine='python', error_bad_lines=False) \n",
    "\n",
    "Xcases_val = cases_val.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycases_val = np.ones(len(Xcases_val.index), dtype=np.int64)\n",
    "\n",
    "Xcontrols_val = controls_val.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycontrols_val = np.zeros(len(Xcontrols_val.index), dtype=np.int64)\n",
    "\n",
    "X_val = np.concatenate((Xcases_val, Xcontrols_val))\n",
    "yy_val = np.concatenate((ycases_val, ycontrols_val))\n",
    "\n",
    "mm = -1\n",
    "for lis in X_val:\n",
    "    if len(lis) > mm:\n",
    "        mm = len(lis)\n",
    "\n",
    "XX_val = np.zeros((len(X_val), mm), dtype=np.int64)\n",
    "for i in range(len(X_val)):\n",
    "    for j in range(len(X_val[i])):\n",
    "        XX_val[i, j] = int(X_val[i][j])\n",
    "XX[0]\n",
    "print(f'Number of features {mm}')\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_val, y_val = shuffle(XX_val, yy_val, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366f5c1",
   "metadata": {},
   "source": [
    "## Training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing  training set\n",
    "cases_fold1 = pd.read_csv(\"Cases_sequence_fold3.csv\")\n",
    "cases_fold1 = cases_fold1[~cases_fold1['person_id'].isin(excludeIds['person_id'])]\n",
    "controls_fold1 = pd.read_csv(\"Controls_sequence_fold3.csv\",engine='python', error_bad_lines=False)\n",
    "controls_fold1 = controls_fold1.dropna()\n",
    "cases_fold2 = pd.read_csv(\"Cases_sequence_fold4.csv\")\n",
    "cases_fold2 = cases_fold2[~cases_fold2['person_id'].isin(excludeIds['person_id'])]\n",
    "controls_fold2 = pd.read_csv(\"Controls_sequence_fold4.csv\",engine='python', error_bad_lines=False)\n",
    "controls_fold2 = controls_fold2.dropna()\n",
    "cases_fold3 = pd.read_csv(\"Cases_sequence_fold5.csv\")\n",
    "cases_fold3 = cases_fold3[~cases_fold3['person_id'].isin(excludeIds['person_id'])]\n",
    "controls_fold3 = pd.read_csv(\"Controls_sequence_fold5.csv\",engine='python', error_bad_lines=False)\n",
    "controls_fold3 = controls_fold3.dropna()\n",
    "\n",
    "Xcases_train_fold1 = cases_fold1.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycases_train_fold1 = np.ones(len(Xcases_train_fold1.index), dtype=np.int64)\n",
    "Xcases_train_fold2 = cases_fold2.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycases_train_fold2 = np.ones(len(Xcases_train_fold2.index), dtype=np.int64)\n",
    "Xcases_train_fold3 = cases_fold3.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycases_train_fold3 = np.ones(len(Xcases_train_fold3.index), dtype=np.int64)\n",
    "\n",
    "Xcontrols_train_fold1 = controls_fold1.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycontrols_train_fold1 = np.zeros(len(Xcontrols_train_fold1.index), dtype=np.int64)\n",
    "Xcontrols_train_fold2 = controls_fold2.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycontrols_train_fold2 = np.zeros(len(Xcontrols_train_fold2.index), dtype=np.int64)\n",
    "Xcontrols_train_fold3 = controls_fold3.groupby(['person_id'], as_index=False).agg({'concept_id':lambda x: list(x) })['concept_id']\n",
    "ycontrols_train_fold3 = np.zeros(len(Xcontrols_train_fold3.index), dtype=np.int64)\n",
    "\n",
    "X_train = np.concatenate((Xcases_train_fold1, Xcontrols_train_fold1, Xcases_train_fold2, Xcontrols_train_fold2, Xcases_train_fold3, Xcontrols_train_fold3))\n",
    "yy_train = np.concatenate((ycases_train_fold1, ycontrols_train_fold1, ycases_train_fold2, ycontrols_train_fold2, ycases_train_fold3, ycontrols_train_fold3))\n",
    "\n",
    "mm = -1\n",
    "for lis in X_train:\n",
    "    if len(lis) > mm:\n",
    "        mm = len(lis)\n",
    "\n",
    "XX_train = np.zeros((len(X_train), mm), dtype=np.int64)\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(X_train[i])):\n",
    "        XX_train[i, j] = int(X_train[i][j])\n",
    "\n",
    "print(f'Number of features {mm}')\n",
    "        \n",
    "        \n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(XX_train, yy_train, random_state=0)\n",
    "\n",
    "feat_X_train = X_train.shape[1]\n",
    "feat_X_val = X_val.shape[1]\n",
    "feat_X_test = X_test.shape[1]\n",
    "max_feat = max(feat_X_train, feat_X_val, feat_X_test)\n",
    "X_train = np.pad(X_train, pad_width=((0, 0), (0, max_feat - feat_X_train)), mode='constant')\n",
    "X_val = np.pad(X_val, pad_width=((0, 0), (0, max_feat - feat_X_val)), mode='constant')\n",
    "X_test = np.pad(X_test, pad_width=((0, 0), (0, max_feat - feat_X_test)), mode='constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c4f2b",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f77fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter search on validation set\n",
    "best_val = -1\n",
    "best_params = {}\n",
    "nums_estimators = np.arange(70,221,20)\n",
    "maxs_features = [\"sqrt\"]\n",
    "max_depths = [None]\n",
    "for num_estim in nums_estimators:\n",
    "    for max_feat in maxs_features:\n",
    "        for max_dep in max_depths:\n",
    "            regressor = RandomForestRegressor(n_estimators=num_estim, max_features = max_feat, max_depth = max_dep, random_state=0)\n",
    "            regressor.fit(X_train, y_train)\n",
    "            val_pred = regressor.predict(X_val)\n",
    "            auc = roc_auc_score(y_val, val_pred)\n",
    "            if auc > best_val:\n",
    "                best_val = auc\n",
    "                best_params[\"n_estimators\"] = num_estim\n",
    "                best_params[\"max_features\"] = max_feat\n",
    "                best_params[\"max_depth\"] = max_dep\n",
    "best_params, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef252eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[\"max_depth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00922da",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_estim = best_params[\"n_estimators\"]\n",
    "best_max_feat = best_params[\"max_features\"]\n",
    "best_max_dep = best_params[\"max_depth\"] \n",
    "regressor = RandomForestRegressor(n_estimators = best_num_estim, max_features = best_max_feat, max_depth = best_max_dep, random_state=0)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest model saving\n",
    "fold_save = \"1\"\n",
    "save_name = \"models/random_forest_\" + fold_save + \".joblib\"\n",
    "loaded_model = joblib.load(save_name)\n",
    "result = loaded_model.predict(X_test)\n",
    "roc_auc_score(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b194be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "y_pred = result\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "auc_keras\n",
    "\n",
    "fold_num_print = 'Fold_num_1'\n",
    "fold_title = 'Fold num 1'\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='AUC = {:.3f}'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ML - ROC curve - ' + str(fold_title))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('cv/PAD_ML_auc_curve_'+str(fold_num_print)+'.jpeg')\n",
    "plt.show()\n",
    "\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(y_test, y_pred, n_bins=10)\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "plt.plot(mpv, fop, marker='.')\n",
    "plt.xlabel('True probabilities')\n",
    "plt.ylabel('Predicted probabilities')\n",
    "plt.title('ML - Calibration curve - '+ str(fold_title))\n",
    "plt.savefig('cv/PAD_ML_calibration_'+fold_num_print+'.jpeg')\n",
    "plt.show()\n",
    "\n",
    "gmeans = np.sqrt(tpr_keras * (1-fpr_keras))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "optimal_threshold = thresholds_keras[ix]\n",
    "print(\"optimal_threshold = \", optimal_threshold)\n",
    "\n",
    "cm=confusion_matrix(y_test, y_pred>optimal_threshold)\n",
    "TP = cm[0][0]\n",
    "FN = cm[0][1]\n",
    "FP = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "cm,TP,TN,FP,FN,auc_keras,TNR,TPR\n",
    "\n",
    "print(\"AUC = \", auc_keras)\n",
    "print(\"Specificity = \", TNR)\n",
    "print(\"Sensitivity = \", TPR)\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "alpha = .95\n",
    "y_true = y_test\n",
    "\n",
    "auc, auc_cov = delong_roc_variance(\n",
    "    y_true,\n",
    "    y_pred)\n",
    "\n",
    "auc_std = np.sqrt(auc_cov)\n",
    "lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "ci = stats.norm.ppf(\n",
    "    lower_upper_q,\n",
    "    loc=auc,\n",
    "    scale=auc_std)\n",
    "\n",
    "ci[ci > 1] = 1\n",
    "\n",
    "print('AUC:', auc)\n",
    "# print('AUC COV:', auc_cov)\n",
    "print('95% AUC CI:', ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6234a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test, y_pred>optimal_threshold)\n",
    "TP = cm[0][0]\n",
    "FN = cm[0][1]\n",
    "FP = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084001bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 1\n",
    "pd.DataFrame(fpr_keras).to_csv(\"plots/fpr\" + str(fold_num) + \".csv\")\n",
    "pd.DataFrame(tpr_keras).to_csv(\"plots/tpr\" + str(fold_num) + \".csv\")\n",
    "pd.DataFrame(fop).to_csv(\"plots/fop\" + str(fold_num) + \".csv\")\n",
    "pd.DataFrame(mpv).to_csv(\"plots/mpv\" + str(fold_num) + \".csv\")\n",
    "pd.DataFrame([auc_keras]).to_csv(\"plots/auc\" + str(fold_num) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'auc': auc_keras, 'specificity': TNR, 'sensitivity': TPR}\n",
    "df = pd.DataFrame(data=d, index = [0], dtype=np.float64)\n",
    "df.to_csv('cv/auc_specificity_sensitivity_fold1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC = \", auc_keras)\n",
    "print(\"Specificity = \", TNR)\n",
    "print(\"Sensitivity = \", TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56419fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
