{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fcc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting up the python environment\n",
    "import pandas as pd\n",
    "import os \n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "%matplotlib inline \n",
    "import textwrap\n",
    "\n",
    "##Set up BQ API\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "##Set up Google sdk environment\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '***'\n",
    "## Connect to the right GCP project\n",
    "os.environ['GCLOUD_PROJECT'] = '***' \n",
    "%load_ext google.cloud.bigquery\n",
    "client=bigquery.Client()\n",
    "\n",
    "project_id = \"***\"\n",
    "# dataset_id = \"***\"\n",
    "dataset_id = \"***\"\n",
    "work_project_id = '***' \n",
    "work_dataset_id = '***'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac29df1",
   "metadata": {},
   "source": [
    "# Nomogram scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c00e8c",
   "metadata": {},
   "source": [
    "## Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "CREATE OR REPLACE TABLE `{work_project_id}.{work_dataset_id}.Cases_FM_temp_nom` AS\n",
    "SELECT *, \n",
    "(CASE \n",
    "WHEN 45 <= Age AND Age <= 49 THEN 0\n",
    "WHEN 50 <= Age AND Age <= 54 THEN 1\n",
    "WHEN 55 <= Age AND Age <= 59 THEN 2\n",
    "WHEN 60 <= Age AND Age <= 64 THEN 3\n",
    "WHEN 65 <= Age AND Age <= 69 THEN 4\n",
    "WHEN 70 <= Age AND Age <= 74 THEN 5\n",
    "WHEN 75 <= Age AND Age <= 79 THEN 6\n",
    "WHEN 80 <= Age AND Age <= 84 THEN 7\n",
    "WHEN 85 <= Age AND Age <= 89 THEN 8\n",
    "WHEN 90 <= Age AND Age <= 94 THEN 9\n",
    "WHEN 95 <= Age AND Age <= 99 THEN 10\n",
    "WHEN 100 <= Age THEN 11\n",
    "END) AS age_score,\n",
    "(CASE \n",
    "WHEN Female = 0 THEN 0\n",
    "WHEN Female = 1 THEN 4\n",
    "ELSE 0\n",
    "END) AS sex_score,\n",
    "(CASE \n",
    "WHEN Race IN ('Black', 'Other') THEN 16\n",
    "WHEN Race IN ('Caucasian', 'Hispanic') THEN 15\n",
    "WHEN Race IN ('Asian') THEN 0\n",
    "ELSE 15\n",
    "END) AS race_score,\n",
    "(CASE WHEN Diab = 1 THEN 4\n",
    "ELSE 0\n",
    "END) AS diab_score,\n",
    "(CASE \n",
    "WHEN BMI <= 23 THEN 10\n",
    "WHEN 23 <= BMI AND BMI <= 24 THEN 9\n",
    "WHEN 24 <= BMI AND BMI <= 26 THEN 8\n",
    "WHEN 26 <= BMI AND BMI <= 27 THEN 7\n",
    "WHEN 27 <= BMI AND BMI <= 29 THEN 6\n",
    "WHEN 29 <= BMI AND BMI <= 31 THEN 5\n",
    "WHEN 31 <= BMI AND BMI <= 32 THEN 4\n",
    "WHEN 32 <= BMI AND BMI <= 34 THEN 3\n",
    "WHEN 34 <= BMI AND BMI <= 35 THEN 2\n",
    "WHEN 35 <= BMI AND BMI <= 38 THEN 1\n",
    "WHEN 38 <= BMI  THEN 0\n",
    "ELSE 6\n",
    "END) AS bmi_score,\n",
    "(CASE WHEN HTN = 1 THEN 2\n",
    "ELSE 0\n",
    "END) AS htn_score,\n",
    "(CASE WHEN new_Smoking_status = 'Current' THEN 13\n",
    "ELSE 0\n",
    "END) AS smoking_score,\n",
    "(CASE WHEN CAD = 1 THEN 2\n",
    "ELSE 0\n",
    "END) AS cad_score,\n",
    "(CASE WHEN CVA = 1 THEN 3\n",
    "ELSE 0\n",
    "END) AS cva_score,\n",
    "(CASE WHEN HF = 1 THEN 9\n",
    "ELSE 0\n",
    "END) AS hf_score,\n",
    "FROM `{work_project_id}.{work_dataset_id}.Cases_FM_temp_smok` \n",
    "\"\"\".format_map({\n",
    "                'project_id': project_id,\n",
    "                'dataset_id': dataset_id,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "                'fold_num': fold_num})\n",
    "df = client.query(query).to_dataframe()\n",
    "df\n",
    "\n",
    "query=\"\"\"\n",
    "CREATE OR REPLACE TABLE `{work_project_id}.{work_dataset_id}.Cases_FM_temp_nom` AS\n",
    "SELECT *, (age_score + sex_score + race_score + diab_score + bmi_score + htn_score + smoking_score + cad_score + cva_score + hf_score) \n",
    "AS nomogram_score\n",
    "FROM `{work_project_id}.{work_dataset_id}.Cases_FM_temp_nom` \n",
    "\"\"\".format_map({\n",
    "                'project_id': project_id,\n",
    "                'dataset_id': dataset_id,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "                'fold_num': fold_num})\n",
    "df = client.query(query).to_dataframe()\n",
    "df\n",
    "\n",
    "#SELECT person_id, PAD, Age, Female, Race, CVA, CAD, HF, HTN, Diab, HLD, BMI, new_Smoking_status, nomogram_score\n",
    "query=\"\"\"\n",
    "CREATE OR REPLACE TABLE `{work_project_id}.{work_dataset_id}.Cases_FM_temp_nom` AS\n",
    "SELECT person_id, nomogram_score, 1 AS label\n",
    "FROM `{work_project_id}.{work_dataset_id}.Cases_FM_temp_nom`\n",
    "WHERE person_id NOT IN (\n",
    "SELECT *\n",
    "FROM `{work_project_id}.{work_dataset_id}.321052_ids`)\n",
    "\"\"\".format_map({\n",
    "                'project_id': project_id,\n",
    "                'dataset_id': dataset_id,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "                'fold_num': fold_num})\n",
    "df = client.query(query).to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14703833",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "SELECT *\n",
    "FROM `{work_project_id}.{work_dataset_id}.Cases_FM_temp_nom` \n",
    "\"\"\".format_map({\n",
    "                'project_id': project_id,\n",
    "                'dataset_id': dataset_id,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "                'fold_num': fold_num})\n",
    "df = client.query(query).to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf40dc0",
   "metadata": {},
   "source": [
    "## Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb80db",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "CREATE OR REPLACE TABLE `{work_project_id}.{work_dataset_id}.Controls_FM_temp_nom` AS\n",
    "SELECT *, \n",
    "(CASE \n",
    "WHEN 45 <= Age AND Age <= 49 THEN 0\n",
    "WHEN 50 <= Age AND Age <= 54 THEN 1\n",
    "WHEN 55 <= Age AND Age <= 59 THEN 2\n",
    "WHEN 60 <= Age AND Age <= 64 THEN 3\n",
    "WHEN 65 <= Age AND Age <= 69 THEN 4\n",
    "WHEN 70 <= Age AND Age <= 74 THEN 5\n",
    "WHEN 75 <= Age AND Age <= 79 THEN 6\n",
    "WHEN 80 <= Age AND Age <= 84 THEN 7\n",
    "WHEN 85 <= Age AND Age <= 89 THEN 8\n",
    "WHEN 90 <= Age AND Age <= 94 THEN 9\n",
    "WHEN 95 <= Age AND Age <= 99 THEN 10\n",
    "WHEN 100 <= Age THEN 11\n",
    "END) AS age_score,\n",
    "(CASE \n",
    "WHEN Female = 0 THEN 0\n",
    "WHEN Female = 1 THEN 4\n",
    "ELSE 0\n",
    "END) AS sex_score,\n",
    "(CASE \n",
    "WHEN Race IN ('Black', 'Other') THEN 16\n",
    "WHEN Race IN ('Caucasian', 'Hispanic') THEN 15\n",
    "WHEN Race IN ('Asian') THEN 0\n",
    "ELSE 15\n",
    "END) AS race_score,\n",
    "(CASE WHEN Diab = 1 THEN 4\n",
    "ELSE 0\n",
    "END) AS diab_score,\n",
    "(CASE \n",
    "WHEN BMI <= 23 THEN 10\n",
    "WHEN 23 <= BMI AND BMI <= 24 THEN 9\n",
    "WHEN 24 <= BMI AND BMI <= 26 THEN 8\n",
    "WHEN 26 <= BMI AND BMI <= 27 THEN 7\n",
    "WHEN 27 <= BMI AND BMI <= 29 THEN 6\n",
    "WHEN 29 <= BMI AND BMI <= 31 THEN 5\n",
    "WHEN 31 <= BMI AND BMI <= 32 THEN 4\n",
    "WHEN 32 <= BMI AND BMI <= 34 THEN 3\n",
    "WHEN 34 <= BMI AND BMI <= 35 THEN 2\n",
    "WHEN 35 <= BMI AND BMI <= 38 THEN 1\n",
    "WHEN 38 <= BMI  THEN 0\n",
    "ELSE 6\n",
    "END) AS bmi_score,\n",
    "(CASE WHEN HTN = 1 THEN 2\n",
    "ELSE 0\n",
    "END) AS htn_score,\n",
    "(CASE WHEN Smoking_status = 'Current' THEN 13\n",
    "ELSE 0\n",
    "END) AS smoking_score,\n",
    "(CASE WHEN CAD = 1 THEN 2\n",
    "ELSE 0\n",
    "END) AS cad_score,\n",
    "(CASE WHEN CVA = 1 THEN 3\n",
    "ELSE 0\n",
    "END) AS cva_score,\n",
    "(CASE WHEN HF = 1 THEN 9\n",
    "ELSE 0\n",
    "END) AS hf_score,\n",
    "FROM `{work_project_id}.{work_dataset_id}.Controls_FM_temp_smok` \n",
    "\"\"\".format_map({\n",
    "                'project_id': project_id,\n",
    "                'dataset_id': dataset_id,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "                'fold_num': fold_num})\n",
    "df = client.query(query).to_dataframe()\n",
    "df\n",
    "\n",
    "query=\"\"\"\n",
    "CREATE OR REPLACE TABLE `{work_project_id}.{work_dataset_id}.Controls_FM_temp_nom` AS\n",
    "SELECT *, (age_score + sex_score + race_score + diab_score + bmi_score + htn_score + smoking_score + cad_score + cva_score + hf_score) \n",
    "AS nomogram_score\n",
    "FROM `{work_project_id}.{work_dataset_id}.Controls_FM_temp_nom` \n",
    "\"\"\".format_map({\n",
    "                'project_id': project_id,\n",
    "                'dataset_id': dataset_id,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "                'fold_num': fold_num})\n",
    "df = client.query(query).to_dataframe()\n",
    "df\n",
    "\n",
    "query=\"\"\"\n",
    "CREATE OR REPLACE TABLE `{work_project_id}.{work_dataset_id}.Controls_FM_temp_nom` AS\n",
    "SELECT person_id, nomogram_score, 0 AS label\n",
    "FROM `{work_project_id}.{work_dataset_id}.Controls_FM_temp_nom` \n",
    "WHERE person_id NOT IN (\n",
    "SELECT *\n",
    "FROM `{work_project_id}.{work_dataset_id}.321052_ids`)\n",
    "\"\"\".format_map({\n",
    "                'project_id': project_id,\n",
    "                'dataset_id': dataset_id,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "                'fold_num': fold_num})\n",
    "df = client.query(query).to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 1\n",
    "query=\"\"\"\n",
    "(SELECT a.*\n",
    "FROM `{work_project_id}.{work_dataset_id}.Cases_FM_temp_nom` a\n",
    "JOIN `{work_project_id}.{work_dataset_id}.person_ids_cases_sequence_fold_{fold_num}` b\n",
    "ON a.person_id = b.person_id)\n",
    "UNION ALL\n",
    "(SELECT a.*\n",
    "FROM `{work_project_id}.{work_dataset_id}.Controls_FM_temp_nom` a\n",
    "JOIN `{work_project_id}.{work_dataset_id}.person_ids_controls_sequence_fold_{fold_num}` b\n",
    "ON a.person_id = b.person_id)\n",
    "\"\"\".format_map({\n",
    "                'project_id': project_id,\n",
    "                'dataset_id': dataset_id,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "                'fold_num': fold_num})\n",
    "df = client.query(query).to_dataframe()\n",
    "df\n",
    "df.to_csv(\"data_nom_{fold_num}.csv\".format_map({'fold_num': fold_num}), index=False, header=True)\n",
    "\n",
    "df = pd.read_csv(\"data_nom_{fold_num}.csv\".format_map({'fold_num': fold_num}))\n",
    "df.head()\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.drop(\"person_id\",axis = 1)\n",
    "\n",
    "X_nom = df.drop(\"label\",axis = 1) \n",
    "y_nom = df.label\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_nom, X_test_nom, y_train_nom, y_test_nom = train_test_split(X_nom, y_nom, test_size=0.15, random_state=42)\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train_nom, y_train_nom)\n",
    "y_pred_nom = logistic_regression.predict_proba(X_test_nom)[:,1]\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds_nom = roc_curve(y_test_nom,y_pred_nom)\n",
    "auc = auc(fpr, tpr)\n",
    "auc\n",
    "\n",
    "fold_num_print = 'Fold_num_' + str(fold_num)\n",
    "fold_title = 'Fold num ' + str(fold_num)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Nomogram Model - ROC curve - ' + str(fold_title))\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('PAD_Nom_auc_curve_'+str(fold_num_print)+'.jpeg')\n",
    "plt.show()\n",
    "\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(y_test_nom, y_pred_nom, n_bins=10)\n",
    "# plot perfectly calibrated\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "plt.plot(mpv, fop, marker='.')\n",
    "plt.xlabel('True probabilities')\n",
    "plt.ylabel('Predicted probabilities')\n",
    "plt.title('Nomogram Model - Calibration curve - '+ str(fold_title))\n",
    "plt.savefig('PAD_Nom_calibration_'+fold_num_print+'.jpeg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "optimal_threshold = thresholds_nom[ix]\n",
    "print(\"optimal_threshold = \", optimal_threshold)\n",
    "\n",
    "cm=confusion_matrix(y_test_nom, y_pred_nom>optimal_threshold)\n",
    "TP = cm[0][0]\n",
    "FN = cm[0][1]\n",
    "FP = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "cm,TP,TN,FP,FN,auc,TNR,TPR\n",
    "\n",
    "print(\"AUC = \", auc)\n",
    "print(\"Specificity = \", TNR)\n",
    "print(\"Sensitivity = \", TPR)\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "alpha = .95\n",
    "# y_pred = np.array([0.21, 0.32, 0.63, 0.35, 0.92, 0.79, 0.82, 0.99, 0.04])\n",
    "y_true = y_test_nom\n",
    "\n",
    "auc_delong, auc_cov = delong_roc_variance(\n",
    "    y_true,\n",
    "    y_pred_nom)\n",
    "\n",
    "auc_std = np.sqrt(auc_cov)\n",
    "lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "ci = stats.norm.ppf(\n",
    "    lower_upper_q,\n",
    "    loc=auc_delong,\n",
    "    scale=auc_std)\n",
    "\n",
    "ci[ci > 1] = 1\n",
    "\n",
    "print('AUC:', auc_delong)\n",
    "# print('AUC COV:', auc_cov)\n",
    "print('95% AUC CI:', ci)\n",
    "print('AUC STD:', auc_std)\n",
    "\n",
    "d = {'auc': auc, 'specificity': TNR, 'sensitivity': TPR}\n",
    "df = pd.DataFrame(data=d, index = [0], dtype=np.float64)\n",
    "df.to_csv('nom_auc_specificity_sensitivity_fold'+ str(fold_num) +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd794d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs = []\n",
    "specificitys = []\n",
    "sensitivitys = []\n",
    "\n",
    "for i in range(5):\n",
    "    df = pd.read_csv('nom_auc_specificity_sensitivity_fold' + str(i+1) + '.csv')  \n",
    "    aucs.append(float(df['auc']))\n",
    "    specificitys.append(float(df['specificity']))\n",
    "    sensitivitys.append(float(df['sensitivity']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"{:.3f}\".format(v) for v in aucs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5089948",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"{:.3f}\".format(v) for v in specificitys] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d033f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"{:.3f}\".format(v) for v in sensitivitys] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(aucs)\n",
    "\"{:.3f}\".format(np.mean(aucs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(specificitys)\n",
    "\"{:.3f}\".format(np.mean(specificitys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sensitivitys)\n",
    "\"{:.3f}\".format(np.mean(sensitivitys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b54f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0333ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
