{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "Cases_sequence_all_files = glob.glob(\"/Cases_sequence_*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in Cases_sequence_all_files:\n",
    "    if \"fold1\" in filename:\n",
    "        continue\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "Cases_sequence_frame = pd.concat(li, axis=0, ignore_index=True,sort=True)\n",
    "\n",
    "Cases_features_all_files = glob.glob(\"/Cases_features_*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in Cases_features_all_files:\n",
    "    if \"fold1\" in filename:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "Cases_features_frame = pd.concat(li, axis=0, ignore_index=True,sort=True)\n",
    "\n",
    "Controls_sequence_all_files = glob.glob(\"/Controls_sequence_*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in Controls_sequence_all_files:\n",
    "    if \"fold1\" in filename:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "Controls_sequence_frame = pd.concat(li, axis=0, ignore_index=True,sort=True)\n",
    "\n",
    "Controls_features_all_files = glob.glob(\"/Controls_features_*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in Controls_features_all_files:\n",
    "    if \"fold1\" in filename:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "Controls_features_frame = pd.concat(li, axis=0, ignore_index=True,sort=True)\n",
    "\n",
    "\n",
    "data_for_rnn_train_feat_PAD = Cases_features_frame\n",
    "data_for_rnn_train_feat_CONT = Controls_features_frame\n",
    "data_for_rnn_train_seq_PAD = Cases_sequence_frame\n",
    "data_for_rnn_train_seq_CONT = Controls_sequence_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 'fold1'\n",
    "fold_num_print = 'Fold num 1'\n",
    "data_for_rnn_test_seq_PAD=pd.read_csv('/Cases_sequence_'+fold_num+'.csv',header=0, skip_blank_lines=True,low_memory=False)\n",
    "data_for_rnn_test_feat_PAD=pd.read_csv('/Cases_features_'+fold_num+'.csv',header=0, skip_blank_lines=True,low_memory=False)\n",
    "data_for_rnn_test_seq_CONT=pd.read_csv('/Controls_sequence_'+fold_num+'.csv',header=0, skip_blank_lines=True,low_memory=False)\n",
    "data_for_rnn_test_feat_CONT=pd.read_csv('/Controls_features_'+fold_num+'.csv',header=0, skip_blank_lines=True,low_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(data_for_rnn_test_feat_CONT['observation_unique'])\n",
    "le = LabelEncoder()\n",
    "le.fit(data_for_rnn_test_feat_CONT['observation_unique'])\n",
    "le.transform(data_for_rnn_test_feat_CONT['observation_unique'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_rnn_train_seq = pd.concat([data_for_rnn_train_seq_PAD,data_for_rnn_train_seq_CONT],axis=0)\n",
    "data_for_rnn_train_feat = pd.concat([data_for_rnn_train_feat_PAD,data_for_rnn_train_feat_CONT],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_rnn_test_seq = pd.concat([data_for_rnn_test_seq_PAD,data_for_rnn_test_seq_CONT],axis=0)\n",
    "data_for_rnn_test_feat = pd.concat([data_for_rnn_test_feat_PAD,data_for_rnn_test_feat_CONT],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sample = pd.DataFrame()\n",
    "test_data_sample['patient'] = data_for_rnn_test_seq['person_id']\n",
    "test_data_sample['code'] = data_for_rnn_test_seq['concept_id']\n",
    "test_data_sample['type'] = data_for_rnn_test_seq['type']\n",
    "test_data_sample['rec'] = data_for_rnn_test_seq['recency']\n",
    "test_data_sample['year'] = data_for_rnn_test_seq['year']\n",
    "test_data_sample['days'] = data_for_rnn_test_seq['distance_days']\n",
    "test_data_sample['ag'] = data_for_rnn_test_seq['age']\n",
    "test_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = pd.DataFrame()\n",
    "train_data_sample['patient'] = data_for_rnn_train_seq['person_id']\n",
    "train_data_sample['code'] = data_for_rnn_train_seq['concept_id']\n",
    "train_data_sample['type'] = data_for_rnn_train_seq['type']\n",
    "train_data_sample['rec'] = data_for_rnn_train_seq['recency']\n",
    "train_data_sample['year'] = data_for_rnn_train_seq['year']\n",
    "train_data_sample['days'] = data_for_rnn_train_seq['distance_days']\n",
    "train_data_sample['ag'] = data_for_rnn_train_seq['age']\n",
    "train_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sample_user = pd.DataFrame()\n",
    "test_data_sample_user['patient'] = data_for_rnn_test_feat['person_id']\n",
    "test_data_sample_user['f1'] = data_for_rnn_test_feat['age']\n",
    "test_data_sample_user['f2'] = data_for_rnn_test_feat['race']\n",
    "test_data_sample_user['f3'] = data_for_rnn_test_feat['gender']\n",
    "test_data_sample_user['f4'] = data_for_rnn_test_feat['condition_unique']\n",
    "test_data_sample_user['f5'] = data_for_rnn_test_feat['procedure_unique']\n",
    "test_data_sample_user['f6'] = data_for_rnn_test_feat['observation_unique']\n",
    "test_data_sample_user['label'] = data_for_rnn_test_feat['label']\n",
    "\n",
    "test_data_sample_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample_user = pd.DataFrame()\n",
    "train_data_sample_user['patient'] = data_for_rnn_train_feat['person_id']\n",
    "train_data_sample_user['f1'] = data_for_rnn_train_feat['age']\n",
    "train_data_sample_user['f2'] = data_for_rnn_train_feat['race']\n",
    "train_data_sample_user['f3'] = data_for_rnn_train_feat['gender']\n",
    "train_data_sample_user['f4'] = data_for_rnn_train_feat['condition_unique']\n",
    "train_data_sample_user['f5'] = data_for_rnn_train_feat['procedure_unique']\n",
    "train_data_sample_user['f6'] = data_for_rnn_train_feat['observation_unique']\n",
    "train_data_sample_user['label'] = data_for_rnn_train_feat['label']\n",
    "\n",
    "train_data_sample_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test_train = pd.concat([train_data_sample['code'],test_data_sample['code']])\n",
    "pd_test_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(pd.concat([train_data_sample['code'],test_data_sample['code']]))\n",
    "le_tp = LabelEncoder()\n",
    "le_tp.fit(pd.concat([train_data_sample['type'],test_data_sample['type']]))\n",
    "le_rec = LabelEncoder()\n",
    "le_rec.fit(pd.concat([train_data_sample['rec'],test_data_sample['rec']]))\n",
    "le_yr = LabelEncoder()\n",
    "le_yr.fit(pd.concat([train_data_sample['year'],test_data_sample['year']]))\n",
    "le_dy = LabelEncoder()\n",
    "le_dy.fit(pd.concat([train_data_sample['days'],test_data_sample['days']]))\n",
    "le_ag = LabelEncoder()\n",
    "le_ag.fit(pd.concat([train_data_sample['ag'],test_data_sample['ag']]))\n",
    "le_f1 = LabelEncoder()\n",
    "le_f1.fit(pd.concat([train_data_sample_user['f1'],test_data_sample_user['f1']]))\n",
    "le_f2 = LabelEncoder()\n",
    "le_f2.fit(pd.concat([train_data_sample_user['f2'],test_data_sample_user['f2']]))\n",
    "le_f3 = LabelEncoder()\n",
    "le_f3.fit(pd.concat([train_data_sample_user['f3'],test_data_sample_user['f3']]))\n",
    "le_f4 = LabelEncoder()\n",
    "le_f4.fit(pd.concat([train_data_sample_user['f4'],test_data_sample_user['f4']]))\n",
    "le_f5 = LabelEncoder()\n",
    "le_f5.fit(pd.concat([train_data_sample_user['f5'],test_data_sample_user['f5']]))\n",
    "le_f6 = LabelEncoder()\n",
    "le_f6.fit(pd.concat([train_data_sample_user['f6'],test_data_sample_user['f6']]))\n",
    "\n",
    "train_data_sample['code'] = le.transform( train_data_sample['code'])\n",
    "train_data_sample['type'] = le_tp.transform( train_data_sample['type'])\n",
    "train_data_sample['rec'] = le_rec.transform( train_data_sample['rec'])\n",
    "train_data_sample['year'] = le_yr.transform( train_data_sample['year'])\n",
    "train_data_sample['days'] = le_dy.transform( train_data_sample['days'])\n",
    "train_data_sample['ag'] = le_ag.transform( train_data_sample['ag'])\n",
    "\n",
    "train_data_sample_user['f1'] = le_f1.transform( train_data_sample_user['f1'])\n",
    "train_data_sample_user['f2'] = le_f2.transform( train_data_sample_user['f2'])\n",
    "train_data_sample_user['f3'] = le_f3.transform( train_data_sample_user['f3'])\n",
    "train_data_sample_user['f4'] = le_f4.transform( train_data_sample_user['f4'])\n",
    "train_data_sample_user['f5'] = le_f5.transform( train_data_sample_user['f5'])\n",
    "train_data_sample_user['f6'] = le_f6.transform( train_data_sample_user['f6'])\n",
    "\n",
    "\n",
    "test_data_sample['code'] = le.transform( test_data_sample['code'])\n",
    "test_data_sample['type'] = le_tp.transform( test_data_sample['type'])\n",
    "test_data_sample['rec'] = le_rec.transform( test_data_sample['rec'])\n",
    "test_data_sample['year'] = le_yr.transform( test_data_sample['year'])\n",
    "test_data_sample['days'] = le_dy.transform( test_data_sample['days'])\n",
    "test_data_sample['ag'] = le_ag.transform( test_data_sample['ag'])\n",
    "\n",
    "test_data_sample_user['f1'] = le_f1.transform( test_data_sample_user['f1'])\n",
    "test_data_sample_user['f2'] = le_f2.transform( test_data_sample_user['f2'])\n",
    "test_data_sample_user['f3'] = le_f3.transform( test_data_sample_user['f3'])\n",
    "test_data_sample_user['f4'] = le_f4.transform( test_data_sample_user['f4'])\n",
    "test_data_sample_user['f5'] = le_f5.transform( test_data_sample_user['f5'])\n",
    "test_data_sample_user['f6'] = le_f6.transform( test_data_sample_user['f6'])\n",
    "\n",
    "\n",
    "train_data_sample,test_data_sample,train_data_sample_user,test_data_sample_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sample_agg = pd.DataFrame()\n",
    "test_data_sample_agg['patient']=test_data_sample.groupby(['patient'],as_index=False).agg({'code':lambda x: list(x)})['patient']\n",
    "test_data_sample_agg['code']=test_data_sample.groupby(['patient'],as_index=False).agg({'code':lambda x: list(x)})['code']\n",
    "test_data_sample_agg['type']=test_data_sample.groupby(['patient'],as_index=False).agg({'type':lambda x: list(x)})['type']\n",
    "test_data_sample_agg['rec']=test_data_sample.groupby(['patient'],as_index=False).agg({'rec':lambda x: list(x)})['rec']\n",
    "test_data_sample_agg['year']=test_data_sample.groupby(['patient'],as_index=False).agg({'year':lambda x: list(x)})['year']\n",
    "test_data_sample_agg['days']=test_data_sample.groupby(['patient'],as_index=False).agg({'days':lambda x: list(x)})['days']\n",
    "test_data_sample_agg['ag']=test_data_sample.groupby(['patient'],as_index=False).agg({'ag':lambda x: list(x)})['ag']\n",
    "test_data_sample_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample_agg = pd.DataFrame()\n",
    "train_data_sample_agg['patient']=train_data_sample.groupby(['patient'],as_index=False).agg({'code':lambda x: list(x)})['patient']\n",
    "train_data_sample_agg['code']=train_data_sample.groupby(['patient'],as_index=False).agg({'code':lambda x: list(x)})['code']\n",
    "train_data_sample_agg['type']=train_data_sample.groupby(['patient'],as_index=False).agg({'type':lambda x: list(x)})['type']\n",
    "train_data_sample_agg['rec']=train_data_sample.groupby(['patient'],as_index=False).agg({'rec':lambda x: list(x)})['rec']\n",
    "train_data_sample_agg['year']=train_data_sample.groupby(['patient'],as_index=False).agg({'year':lambda x: list(x)})['year']\n",
    "train_data_sample_agg['days']=train_data_sample.groupby(['patient'],as_index=False).agg({'days':lambda x: list(x)})['days']\n",
    "train_data_sample_agg['ag']=train_data_sample.groupby(['patient'],as_index=False).agg({'ag':lambda x: list(x)})['ag']\n",
    "train_data_sample_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.DataFrame(train_data_sample_user.merge(train_data_sample_agg, how='inner', right_on='patient',left_on='patient'))\n",
    "train_data,np.unique(train_data.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.DataFrame(test_data_sample_user.merge(test_data_sample_agg, how='inner', right_on='patient',left_on='patient'))\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_person_ids_order=pd.read_csv('/testids_'+fold_num+'.csv',header=0, skip_blank_lines=True,low_memory=False)\n",
    "\n",
    "test_data=pd.DataFrame(RF_person_ids_order.merge(test_data, how='inner', right_on='patient',left_on='test_person_id')).drop_duplicates('test_person_id')\n",
    "\n",
    "test_data, RF_person_ids_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_rnn_train = train_data\n",
    "data_for_rnn_test = test_data\n",
    "data_for_rnn_train,data_for_rnn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#perform the padding for both prods seq and rec seq\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_input_length = 300\n",
    "padding_value_c=len(le.classes_)+1\n",
    "padding_value_t=len(le_tp.classes_)+1\n",
    "padding_value_r=len(le_rec.classes_)+1\n",
    "padding_value_y=len(le_yr.classes_)+1\n",
    "padding_value_d=len(le_dy.classes_)+1\n",
    "padding_value_a=len(le_ag.classes_)+1\n",
    "input_sequences_train_code = pad_sequences(data_for_rnn_train['code'], maxlen=max_input_length, padding='pre',value=padding_value_c)\n",
    "input_sequences_train_type = pad_sequences(data_for_rnn_train['type'], maxlen=max_input_length, padding='pre',value=padding_value_t)\n",
    "input_sequences_train_rec = pad_sequences(data_for_rnn_train['rec'], maxlen=max_input_length, padding='pre',value=padding_value_r)\n",
    "input_sequences_train_year = pad_sequences(data_for_rnn_train['year'], maxlen=max_input_length, padding='pre',value=padding_value_y)\n",
    "input_sequences_train_days = pad_sequences(data_for_rnn_train['days'], maxlen=max_input_length, padding='pre',value=padding_value_d)\n",
    "input_sequences_train_ag = pad_sequences(data_for_rnn_train['ag'], maxlen=max_input_length, padding='pre',value=padding_value_a)\n",
    "input_sequences_test_code = pad_sequences(data_for_rnn_test['code'], maxlen=max_input_length, padding='pre',value=padding_value_c)\n",
    "input_sequences_test_type = pad_sequences(data_for_rnn_test['type'], maxlen=max_input_length, padding='pre',value=padding_value_t)\n",
    "input_sequences_test_rec = pad_sequences(data_for_rnn_test['rec'], maxlen=max_input_length, padding='pre',value=padding_value_r)\n",
    "input_sequences_test_year = pad_sequences(data_for_rnn_test['year'], maxlen=max_input_length, padding='pre',value=padding_value_t)\n",
    "input_sequences_test_days = pad_sequences(data_for_rnn_test['days'], maxlen=max_input_length, padding='pre',value=padding_value_d)\n",
    "input_sequences_test_ag = pad_sequences(data_for_rnn_test['ag'], maxlen=max_input_length, padding='pre',value=padding_value_a)\n",
    "output_sequences_train = data_for_rnn_train['label']\n",
    "output_sequences_test = data_for_rnn_test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequences_train = data_for_rnn_train['label']\n",
    "output_sequences_test = data_for_rnn_test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded_train = np.array(output_sequences_train).reshape(len(output_sequences_train), 1)\n",
    "integer_encoded_test = np.array(output_sequences_test).reshape(len(output_sequences_test), 1)\n",
    "\n",
    "\n",
    "integer_encoded_train_test = np.concatenate((integer_encoded_train,integer_encoded_test),axis=0)\n",
    "\n",
    "\n",
    "onehot_encoder.fit(integer_encoded_train_test)\n",
    "\n",
    "onehot_OUTPUT_train =onehot_encoder.transform(integer_encoded_train)\n",
    "onehot_OUTPUT_test =onehot_encoder.transform(integer_encoded_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vocab_size = len(onehot_encoder.categories_[0])\n",
    "output_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size_code = len(le.classes_)\n",
    "input_vocab_size_type = len(le_tp.classes_)\n",
    "input_vocab_size_rec = len(le_rec.classes_)\n",
    "input_vocab_size_year = len(le_yr.classes_)\n",
    "input_vocab_size_days = len(le_dy.classes_)\n",
    "input_vocab_size_prod_age = len(le_ag.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size_f1 = len(le_f1.classes_)\n",
    "input_vocab_size_f2 = len(le_f2.classes_)\n",
    "input_vocab_size_f3 = len(le_f3.classes_)\n",
    "input_vocab_size_f4 = len(le_f4.classes_)\n",
    "input_vocab_size_f5 = len(le_f5.classes_)\n",
    "input_vocab_size_f6 = len(le_f6.classes_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, concatenate\n",
    "from tensorflow.keras.layers import LSTM # new! \n",
    "from tensorflow.keras.layers import  Embedding\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import SimpleRNN \n",
    "from tensorflow.keras.layers import GRU  \n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import RNN # new! \n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "emb_size_c=100\n",
    "emb_size_t=5\n",
    "emb_size_r=10\n",
    "emb_size_y=10\n",
    "emb_size_d=10\n",
    "emb_size_a=10\n",
    "hidden_units_seq=600\n",
    "hidden_units_feat=200\n",
    "\n",
    "emb_size_f1=25\n",
    "emb_size_f2=10\n",
    "emb_size_f3=2\n",
    "emb_size_f4=10\n",
    "emb_size_f5=10\n",
    "emb_size_f6=10\n",
    "\n",
    "inp_c = Input(shape=(max_input_length,))\n",
    "inp_t = Input(shape=(max_input_length,))\n",
    "inp_r = Input(shape=(max_input_length,))\n",
    "inp_y = Input(shape=(max_input_length,))\n",
    "inp_d = Input(shape=(max_input_length,))\n",
    "inp_a = Input(shape=(max_input_length,))\n",
    "\n",
    "inp_f1 = Input(shape=(1,))\n",
    "inp_f2 = Input(shape=(1,))\n",
    "inp_f3 = Input(shape=(1,))\n",
    "inp_f4 = Input(shape=(1,))\n",
    "inp_f5 = Input(shape=(1,))\n",
    "inp_f6 = Input(shape=(1,))\n",
    "\n",
    "emb_c = Embedding(padding_value_c+1,emb_size_c, input_length=max_input_length)(inp_c)\n",
    "y1 = Reshape((emb_size_c,max_input_length))(emb_c)\n",
    "\n",
    "lstm_y1= LSTM(hidden_units_seq, dropout = 0.3, recurrent_dropout = 0.3)(y1)\n",
    "\n",
    "emb_t = Embedding(padding_value_t+1,emb_size_t, input_length=max_input_length)(inp_t)\n",
    "y2 = Reshape((emb_size_t,max_input_length))(emb_t)\n",
    "\n",
    "emb_r = Embedding(padding_value_r+1,emb_size_r, input_length=max_input_length)(inp_r)\n",
    "y3 = Reshape((emb_size_r,max_input_length))(emb_r)\n",
    "\n",
    "emb_y = Embedding(padding_value_y+1,emb_size_y, input_length=max_input_length)(inp_y)\n",
    "y4 = Reshape((emb_size_y,max_input_length))(emb_y)\n",
    "\n",
    "emb_d = Embedding(padding_value_d+1,emb_size_d, input_length=max_input_length)(inp_d)\n",
    "y5 = Reshape((emb_size_d,max_input_length))(emb_d)\n",
    "\n",
    "emb_a = Embedding(padding_value_a+1,emb_size_a, input_length=max_input_length)(inp_a)\n",
    "y6 = Reshape((emb_size_a,max_input_length))(emb_a)\n",
    "\n",
    "x=concatenate([y1\n",
    "               , \n",
    "               y2,\n",
    "               y3\n",
    "               ,\n",
    "               y4,\n",
    "               y5,\n",
    "               y6\n",
    "              ],axis=1)\n",
    "\n",
    "\n",
    "lstm_l1= LSTM(hidden_units_seq, dropout = 0.5, recurrent_dropout = 0.5)(y1)\n",
    "\n",
    "\n",
    "x_flat = Flatten()(x)\n",
    "\n",
    "relu_x= Dense(50, activation='relu')(x_flat)\n",
    "\n",
    "x_flat_flat = Flatten()(relu_x)\n",
    "\n",
    "y1_flat = Flatten()(y1)\n",
    "\n",
    "relu_y1= Dense(50, activation='relu')(y1_flat)\n",
    "\n",
    "y1_flat_flat = Flatten()(relu_y1)\n",
    "\n",
    "\n",
    "emb_f1 = Embedding(input_vocab_size_f1+1,emb_size_f1, input_length=1)(inp_f1)\n",
    "f1 = Reshape((emb_size_f1,1))(emb_f1)\n",
    "\n",
    "emb_f2 = Embedding(input_vocab_size_f2+1,emb_size_f2, input_length=1)(inp_f2)\n",
    "f2 = Reshape((emb_size_f2,1))(emb_f2)\n",
    "\n",
    "emb_f3 = Embedding(input_vocab_size_f3+1,emb_size_f3, input_length=1)(inp_f3)\n",
    "f3 = Reshape((emb_size_f3,1))(emb_f3)\n",
    "\n",
    "emb_f4 = Embedding(input_vocab_size_f4+1,emb_size_f4, input_length=1)(inp_f4)\n",
    "f4 = Reshape((emb_size_f4,1))(emb_f4)\n",
    "\n",
    "emb_f5 = Embedding(input_vocab_size_f5+1,emb_size_f5, input_length=1)(inp_f5)\n",
    "f5 = Reshape((emb_size_f5,1))(emb_f5)\n",
    "\n",
    "emb_f6 = Embedding(input_vocab_size_f6+1,emb_size_f6, input_length=1)(inp_f6)\n",
    "f6 = Reshape((emb_size_f6,1))(emb_f6)\n",
    "\n",
    "\n",
    "\n",
    "f1_2_conc = concatenate([f1\n",
    "                         ,f2\n",
    "                         ,f3\n",
    "                         ,f4,f5,f6\n",
    "                        ],axis=1)\n",
    "\n",
    "flatten_f1_2_conc = Flatten()(f1_2_conc)\n",
    "relu_f1_2 = Dense(hidden_units_feat, activation='relu')(flatten_f1_2_conc)\n",
    "\n",
    "flatten_f1 = Flatten()(f1)\n",
    "relu_f1 = Dense(hidden_units_feat, activation='relu')(flatten_f1)\n",
    "\n",
    "\n",
    "\n",
    "lstm1_f = concatenate([\n",
    "                        lstm_y1\n",
    "                       , relu_f1_2\n",
    "],axis=1)\n",
    "predictions = Dense(2, activation='sigmoid')(lstm1_f)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[inp_c\n",
    "            ,\n",
    "            inp_t,\n",
    "            inp_r,\n",
    "            inp_y,\n",
    "            inp_d,\n",
    "            inp_a\n",
    "            ,\n",
    "            inp_f1\n",
    "            ,inp_f2\n",
    "            ,inp_f3\n",
    "            ,inp_f4,inp_f5,inp_f6\n",
    "           ],\n",
    "    outputs=predictions\n",
    ")\n",
    "\n",
    "\n",
    "sp = tf.keras.metrics.SpecificityAtSensitivity(0.55,200)\n",
    "sp.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n",
    "sn = tf.keras.metrics.SensitivityAtSpecificity(0.5,200)\n",
    "sn.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n",
    "tp=tf.keras.metrics.TruePositives()\n",
    "tn = tf.keras.metrics.TrueNegatives()\n",
    "fp = tf.keras.metrics.FalsePositives()\n",
    "fn = tf.keras.metrics.FalseNegatives()\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=[\n",
    "                  tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    "    name=None,\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=False,\n",
    "    label_weights=None,\n",
    "),\n",
    "                       sp, #see definition of the SpecificityAtSensitivity\n",
    "                  sn #see definition of the SensitivityAtSpecificity\n",
    "                  ,tn,tp,fn,fp\n",
    "                      ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=2, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model.fit([pd.DataFrame(input_sequences_train_code)\n",
    "           ,\n",
    "           pd.DataFrame(input_sequences_train_type),\n",
    "           pd.DataFrame(input_sequences_train_rec),\n",
    "           pd.DataFrame(input_sequences_train_year),\n",
    "           pd.DataFrame(input_sequences_train_days),\n",
    "           pd.DataFrame(input_sequences_train_ag),\n",
    "           pd.DataFrame(data_for_rnn_train['f1']),\n",
    "           pd.DataFrame(data_for_rnn_train['f2']),\n",
    "           pd.DataFrame(data_for_rnn_train['f3'])\n",
    "           ,\n",
    "           pd.DataFrame(data_for_rnn_train['f4']),\n",
    "           pd.DataFrame(data_for_rnn_train['f5']),\n",
    "           pd.DataFrame(data_for_rnn_train['f6'])\n",
    "          ], onehot_OUTPUT_train,\n",
    "          epochs=20, batch_size =64\n",
    "          , \n",
    "          \n",
    "          callbacks=[earlyStopping],\n",
    "          validation_split=0.1,\n",
    "          verbose=1, shuffle=True\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "\n",
    "y_pred_keras = model.predict([pd.DataFrame(input_sequences_test_code)\n",
    "                              ,\n",
    "           pd.DataFrame(input_sequences_test_type),\n",
    "           pd.DataFrame(input_sequences_test_rec),\n",
    "           pd.DataFrame(input_sequences_test_ag),\n",
    "           pd.DataFrame(data_for_rnn_test['f1']),\n",
    "           pd.DataFrame(data_for_rnn_test['f2']),\n",
    "           pd.DataFrame(data_for_rnn_test['f3'])\n",
    "                              ,\n",
    "           pd.DataFrame(data_for_rnn_test['f4']),\n",
    "           pd.DataFrame(data_for_rnn_test['f5']),\n",
    "           pd.DataFrame(data_for_rnn_test['f6'])\n",
    "                             ])\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
